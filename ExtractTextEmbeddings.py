# -*- coding: utf-8 -*-
"""ExtractTextEmbeddings

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sE2UL7oaAY_I_c8GTmUszlK-o4Piqkss
"""

!pip install transformers
import os
import torch
# !pip install sentencepiece

#use the language model and tokeniser you prefer, BERT, RoBERTa, XLM, XLMRoBERTa from transformer library and modify the code accordingly
from transformers import BertTokenizer, BertModel #RobertaTokenizer, RobertaModel, XLMRobertaTokenizer, TFXLMRobertaModel
import numpy as np
import glob, os
from os import path
import pickle as pkl
from google.colab import drive
drive.mount('/content/drive')

# Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# tokenizer = RobertaTokenizer.from_pretrained('roberta-large')
# tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')


# from transformers import 
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)
# model = TFXLMRobertaModel.from_pretrained('xlm-roberta-base', output_hidden_states = True)


# path = '/content/drive/Shareddrives/NLP_group/Project/Datasetv3/Transcripts'
path ='/content/drive/Shareddrives/NLP_group/Project/Datasetv3/Ted-text' # path to folder containing transcipt txt files
file_save_path = '/content/drive/Shareddrives/NLP_group/Project/Datasetv3/Bertembed/' #destination folder for bert embeddings

counter = 0
filetext = []

#creating Text embeddings
filenames = glob.glob(os.path.join(path, '*.txt'))
filenames = sorted(filenames)
for filename in filenames:
  print(filename)
  with open(filename, 'r') as f:
    temp_text = f.read()
    lst_text = temp_text.strip().split(' ')
    fin_lst_text = ["[CLS]"]
    for word in lst_text:
	    fin_lst_text.append(word)
    fin_text = " ".join(fin_lst_text)
    f.close()
    # print(fin_text)
    tokenized_text = tokenizer.tokenize(fin_text)
    if len(tokenized_text) > 511:
      tokenized_text = tokenized_text[:511]
    tokenized_text.append('[SEP]')
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    segments_ids = [1] * len(tokenized_text)
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    model = model.eval()
    with torch.no_grad():
      outputs = model(tokens_tensor, segments_tensors)
      hidden_states = outputs[2]
    token_embeddings = torch.stack(hidden_states, dim=0)
    token_embeddings = torch.squeeze(token_embeddings, dim=1)
    token_embeddings = token_embeddings.permute(1,0,2)
    token_vecs_sum = []
    for token in token_embeddings:
        # `token` is a [12 x 768] tensor
        # Sum the vectors from the last four layers.
      sum_vec = torch.sum(token[-4:], dim=0)
      sum_vec_np = sum_vec.numpy()
        # Use `sum_vec` to represent `token`.
      token_vecs_sum.append(sum_vec_np)
    if len(token_vecs_sum) < 512:
      for j in range(512-len(token_vecs_sum)):
        padding_vec = np.zeros(768) #change to 1024 in large models
        token_vecs_sum.append(padding_vec)
    bert_file = filename.split('/')[-1]

    bert_file = bert_file.split('.')[0] # Remove .txt from file

    bert_file = file_save_path + bert_file + '_bb.pkl'
    print(bert_file)
    with open(bert_file, 'wb') as f:
      pkl.dump(token_vecs_sum, f)
  counter = counter + 1  
    
print(counter)

#checking the shape of one file
with open('/content/drive/Shareddrives/NLP_group/Project/Datasetv3/BertBaseEmbed/AJ_TP_text_11_bb.pkl', 'rb') as f:
        bert = pkl.load(f)
print(np.shape(bert))

#combining all bertembeddings into a list
path2 = '/content/drive/Shareddrives/NLP_group/Project/Dataset/Combined_data/BERTbaseEmbed/' #folder containing BERT embeddings
filenames = glob.glob(os.path.join(path2, '*_bb.pkl'))
filenames = sorted(filenames)
X_txt_combined = []
counter = 0
for filename in filenames:
  print(filename)
  with open(filename, 'rb') as f:
    bert = pkl.load(f)
    X_txt_combined.append(bert)
  counter = counter + 1
  print(counter)

#saving the list of bert embeddings
path_save = '/content/drive/Shareddrives/NLP_group/Project/Dataset/Combined_data/Importantobjects/'
 
with open(path_save + 'X_txt_bert.pkl', 'wb') as f:
    pkl.dump(X_txt_combined, f)